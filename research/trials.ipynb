{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeStore\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = '8d7fd4ae-327c-4cd5-abbe-0c2d4769e37e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập biến môi trường cho Pinecone API key\n",
    "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extract PDF data\n",
    "# data_path = 'C:/Users/NgLaam/Desktop/chatbot/medical-chatbot/dataset'\n",
    "\n",
    "# def load_pdf(data_path):\n",
    "#     loader = DirectoryLoader(data_path,\n",
    "#                         glob = '*.pdf',\n",
    "#                         loader_cls=PyPDFLoader)\n",
    "    \n",
    "#     documents = loader.load()\n",
    "\n",
    "#     return documents\n",
    "\n",
    "# extract_data = load_pdf(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create text chunks\n",
    "# def text_split(extract_data):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size = 512, chunk_overlap = 20)\n",
    "#     text_chunks = text_splitter.split_documents(extract_data)\n",
    "\n",
    "#     return text_chunks\n",
    "\n",
    "# text_chunks = text_split(extract_data)\n",
    "# print('Length of my chunk is ', len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query = embeddings.embed_query('Xin chào Việt Nam')\n",
    "print('Length', len(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name =\"vni-medical\"\n",
    "\n",
    "# #Creating Embeddings for each of the text chunks and storing \n",
    "# docsearch = PineconeStore.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template=\"\"\"\n",
    "# Use the following pieces of information to answer the user's question.\n",
    "# If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "# Context: {context}\n",
    "# Question: {question}\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "# chain_type_kwargs={'prompt': PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm=CTransformers(model='C:/Users/NgLaam/Desktop/chatbot/medical-chatbot/model/llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
    "#                   model_type='llama',\n",
    "#                   config={'max_new_tokens': 512,\n",
    "#                           'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type='stuff',\n",
    "#     retriever = docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs=chain_type_kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NgLaam\\.cache\\huggingface\\modules\\transformers_modules\\vinai\\PhoGPT-4B-Chat\\116013fa63f8c4025739487e1cbff65b7375bbe2\\configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n",
      "  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n",
      "C:\\Users\\NgLaam\\.cache\\huggingface\\modules\\transformers_modules\\vinai\\PhoGPT-4B-Chat\\116013fa63f8c4025739487e1cbff65b7375bbe2\\configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n",
      "  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n"
     ]
    }
   ],
   "source": [
    "model_path = \"vinai/PhoGPT-4B-Chat\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "config.init_device = \"cpu\"\n",
    "# config.attn_config['attn_impl'] = 'flash' # If installed: this will use either Flash Attention V1 or V2 depending on what is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_directory = \"C:/Users/NgLaam/Desktop/chatbot/medical-chatbot/model/vietnamese7b-llama\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True, cache_dir=cache_directory)\n",
    "# If your GPU does not support bfloat16:\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, cache_dir=cache_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = 'C:/Users/NgLaam/Desktop/chatbot/medical-chatbot/model/vietnamese7b-llama/models--vinai--PhoGPT-4B-Chat/snapshots/116013fa63f8c4025739487e1cbff65b7375bbe2'\n",
    "model = AutoModelForCausalLM.from_pretrained(load_dir, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rét,  chúng  nên  được  dùng  vào  cùng  một  thời  điểm  mỗi  ngày.Thuốc  chống  sốt  rét  chỉ  được  cung  cấp  khi  có  đơn  thuốc  của  bác  sĩ.  Chúng  có  dạng  viên,  viên  nang  và  dạng  tiêm.  Trong  số  các  thuốc  chống  sốt  rét  thường  được  sử  dụng286 BẠC  THÀNH  Y  HỌC  GALE  2Machine Translated by Google\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "\n",
    "# docsearch = PineconeStore.from_existing_index(index_name, embeddings)\n",
    "\n",
    "# query = 'sử dụng thuốc chống sốt rét nên lưu ý điều gì'\n",
    "# similar = docsearch.similarity_search(query, k=3)\n",
    "\n",
    "# # Use a different variable name instead of str\n",
    "# str_similar = str(similar[1])\n",
    "\n",
    "# def clean_text(text):\n",
    "#     # Remove 'page_content=' from the string\n",
    "#     result_str = re.sub(r\"page_content='|\\\\n|'\", \"\", text)\n",
    "#     return result_str\n",
    "\n",
    "# print(clean_text(str_similar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load index if we already have index\n",
    "\n",
    "# def get_answer(docsearch, query, model, max_length = 512):\n",
    "#     concentrated_docs = ''\n",
    "#     docs = docsearch.similarity_search(query, k=2)\n",
    "#     for doc in docs:\n",
    "#         clean_doc  = clean_text(str(doc))\n",
    "#         concentrated_docs += clean_doc\n",
    "    \n",
    "#     print(concentrated_docs)\n",
    "#     input_ids = tokenizer.encode(concentrated_docs, return_tensors='pt')\n",
    "#     sample_outputs = model.generate(input_ids,pad_token_id=tokenizer.eos_token_id,\n",
    "#                                    do_sample=True,\n",
    "#                                    max_length=max_length,\n",
    "#                                    min_length=max_length,\n",
    "#                                    top_k=40,\n",
    "#                                    num_beams=5,\n",
    "#                                    early_stopping=True,\n",
    "#                                    no_repeat_ngram_size=2,\n",
    "#                                    num_return_sequences=1)\n",
    "\n",
    "#     generated_sequences = [tokenizer.decode(output, skip_special_tokens=True) for output in sample_outputs]\n",
    "    \n",
    "#     return list(enumerate(generated_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trị  liệu  hành  vi  xem  Trị  liệu  nhận  thức  hành  viNhiễm  sán  dây  bò  xem  bệnh  sán  dâySự  định  nghĩaSự  đối  đãiSự  miêu  tảhội  chứng  BehcetKHÁCBẠC  THÀNH  Y  HỌC  GALE  2TỔ  CHỨCSÁCH“Lở  loét  trên  giường.”  Bộ  Y  tế  và  Thảo  dược  Hoa  Kỳ.  Ngày  15  tháng  3  năm  1998  <http://www.healthherbs.com>.Ban  cố  vấn  loét  áp  lực  quốc  gia.  SUNY  tại  Buffalo,“Phá  vỡ  kết  nối  giữa  OR  và  loét  áp  lực.”bao  quanh  quầng  vú  (vùng  sẫm  màu  xung  quanh  núm  vú)  và  kéo  dài  xuống  phía  dưới  và  xung  quanh  mặt  dưới  của  vú.  Điều  này  tạo  ra  vết  sẹo  ít  dễ  thấy  nhất.  Các  mô,  mỡ  và  da  thừa  sẽ  được  loại  bỏ,  đồng  thời  núm  vú  và  quầng  vú  được  đặt  lại  vị  trí.Trong  một  số  trường  hợp  nhất  định,  hút  mỡ  (hút  mỡ)  được  sử  dụng  đểNhững  nam  giới  có  bộ  ngực  to  (gynecomastia)  cũng  có  thể  là  ứng  cử\n",
      "[(0, 'Trị  liệu  hành  vi  xem  Trị  liệu  nhận  thức  hành  viNhiễm  sán  dây  bò  xem  bệnh  sán  dâySự  định  nghĩaSự  đối  đãiSự  miêu  tảhội  chứng  BehcetKHÁCBẠC  THÀNH  Y  HỌC  GALE  2TỔ  CHỨCSÁCH“Lở  loét  trên  giường.”  Bộ  Y  tế  và  Thảo  dược  Hoa  Kỳ.  Ngày  15  tháng  3  năm  1998  <http://www.healthherbs.com>.Ban  cố  vấn  loét  áp  lực  quốc  gia.  SUNY  tại  Buffalo,“Phá  vỡ  kết  nối  giữa  OR  và  loét  áp  lực.”bao  quanh  quầng  vú  (vùng  sẫm  màu  xung  quanh  núm  vú)  và  kéo  dài  xuống  phía  dưới  và  xung  quanh  mặt  dưới  của  vú.  Điều  này  tạo  ra  vết  sẹo  ít  dễ  thấy  nhất.  Các  mô,  mỡ  và  da  thừa  sẽ  được  loại  bỏ,  đồng  thời  núm  vú  và  quầng  vú  được  đặt  lại  vị  trí.Trong  một  số  trường  hợp  nhất  định,  hút  mỡ  (hút  mỡ)  được  sử  dụng  đểNhững  nam  giới  có  bộ  ngực  to  (gynecomastia)  cũng  có  thể  là  ứng  cử  cho  nu ếu  chung  với  phụ nữ   thì  đàn ông  thường  bị  kích  thích  tiết  dục  cao  hơn  nhiều  trong  buồng  nách  bẹn  ở  sau  khi  u ết  kế  tràng  Vú.\\n2.3.1. Các yếu tố ảnh hưởng đến sự hình thành và phát triển của một số tế bào ung thư vú, bao gồm:\\n- Yếu tố di truyền: trong gia đình có nhiều người bị u vú thì có nguy cơ mắc bệnh cao hơn so với những người không có gen bị bệnh này. Ngoài ra, khi bố mẹ hoặc người thân có tiền sử mắc các bệnh liên quan đến tuyến vú cũng có khả năng mắc')]\n"
     ]
    }
   ],
   "source": [
    "# query = 'Dị ứng là gì'\n",
    "# result = get_answer(docsearch, query, model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "có  thể  đề  nghị  xét  nghiệm  máu  để  đo  nồng  độ  hormone,nội  tiết  tố  nam,  có  thể  giúp  giảm  lông  trên  cơ  thể.  Nếu  mộtcăng  thẳng  là  hai  nguyên  nhân  chính.  Mất  kinh  thường  làkhối  u.  Việc  chấm  dứt  kinh  nguyệt  cũng  xảy  ra  vĩnh  viễn  sau  khi  mãn  kinh  hoặc  cắt  bỏ  tử  cung.nhưng  việc  khám  nên  bắt  đầu  bằng  que  thử  thai;  khả  năng  mang  thaicó  kinh  nguyệt  bình  thường  trước  đây.  Nhiều  phụ  nữ  thỉnh  thoảng  bị  mất  kinh  mỗi  tháng.  Vô  kinh  xảy  ra  nếu  người  phụ  nữ  mất  kinh  từ  ba  kỳ  kinh  trở  lên  liên  tiếp.Việc  không  có  kinh  nguyệt  được  gọi  là  amenor-rhea.  Vô  kinh  nguyên  phát  là  tình  trạng  không  bắt  đầu  có  kinhRebecca  J.  FreyVô  kinh  có  thể  có  nhiều  nguyên  nhân.  Vô  kinh  nguyên  phát  có  thể  là  kết  quả  của  sự  mất  cân  bằng  nội  tiết  tố,  rối  loạn  tâm  thần,  rối\n",
      "Response : [(0, 'có  thể  đề  nghị  xét  nghiệm  máu  để  đo  nồng  độ  hormone,nội  tiết  tố  nam,  có  thể  giúp  giảm  lông  trên  cơ  thể.  Nếu  mộtcăng  thẳng  là  hai  nguyên  nhân  chính.  Mất  kinh  thường  làkhối  u.  Việc  chấm  dứt  kinh  nguyệt  cũng  xảy  ra  vĩnh  viễn  sau  khi  mãn  kinh  hoặc  cắt  bỏ  tử  cung.nhưng  việc  khám  nên  bắt  đầu  bằng  que  thử  thai;  khả  năng  mang  thaicó  kinh  nguyệt  bình  thường  trước  đây.  Nhiều  phụ  nữ  thỉnh  thoảng  bị  mất  kinh  mỗi  tháng.  Vô  kinh  xảy  ra  nếu  người  phụ  nữ  mất  kinh  từ  ba  kỳ  kinh  trở  lên  liên  tiếp.Việc  không  có  kinh  nguyệt  được  gọi  là  amenor-rhea.  Vô  kinh  nguyên  phát  là  tình  trạng  không  bắt  đầu  có  kinhRebecca  J.  FreyVô  kinh  có  thể  có  nhiều  nguyên  nhân.  Vô  kinh  nguyên  phát  có  thể  là  kết  quả  của  sự  mất  cân  bằng  nội  tiết  tố,  rối  loạn  tâm  thần,  rối loạn thần kinh thực vật.\\nTrong  trường hợp  vô kinh nguyên nhân do  buồng trứng  thì  vẫn  hoạt ộng   nhưng  ít  hơn  so với  những  khác. Do đó  chị em  hiếm  muộn  sẽ  gặp  khó  trong  cuộc  dục  và  nguy ến  sinh  con .  Rối loạn kinh nguyệt ở  cô gái  rất dễ  gây  ảnh hưởng  tới  quan hệ vợ  chồng  làm  suy  trầm ật  dẫn  đến  bế  đình  hạnh  phúc  cho  cả  gia  nhi ều  nu àn  n ớc \\nDo  hiện  chưa  nghiên cứu  kỹ  sâu ạc  vấn  về  quy ết  khoa học  mà  đã  ứng dụng ')]\n",
      "có  thể  đề  nghị  xét  nghiệm  máu  để  đo  nồng  độ  hormone,nội  tiết  tố  nam,  có  thể  giúp  giảm  lông  trên  cơ  thể.  Nếu  mộtcăng  thẳng  là  hai  nguyên  nhân  chính.  Mất  kinh  thường  làkhối  u.  Việc  chấm  dứt  kinh  nguyệt  cũng  xảy  ra  vĩnh  viễn  sau  khi  mãn  kinh  hoặc  cắt  bỏ  tử  cung.nhưng  việc  khám  nên  bắt  đầu  bằng  que  thử  thai;  khả  năng  mang  thaicó  kinh  nguyệt  bình  thường  trước  đây.  Nhiều  phụ  nữ  thỉnh  thoảng  bị  mất  kinh  mỗi  tháng.  Vô  kinh  xảy  ra  nếu  người  phụ  nữ  mất  kinh  từ  ba  kỳ  kinh  trở  lên  liên  tiếp.Việc  không  có  kinh  nguyệt  được  gọi  là  amenor-rhea.  Vô  kinh  nguyên  phát  là  tình  trạng  không  bắt  đầu  có  kinhRebecca  J.  FreyVô  kinh  có  thể  có  nhiều  nguyên  nhân.  Vô  kinh  nguyên  phát  có  thể  là  kết  quả  của  sự  mất  cân  bằng  nội  tiết  tố,  rối  loạn  tâm  thần,  rối\n",
      "Response : [(0, 'có  thể  đề  nghị  xét  nghiệm  máu  để  đo  nồng  độ  hormone,nội  tiết  tố  nam,  có  thể  giúp  giảm  lông  trên  cơ  thể.  Nếu  mộtcăng  thẳng  là  hai  nguyên  nhân  chính.  Mất  kinh  thường  làkhối  u.  Việc  chấm  dứt  kinh  nguyệt  cũng  xảy  ra  vĩnh  viễn  sau  khi  mãn  kinh  hoặc  cắt  bỏ  tử  cung.nhưng  việc  khám  nên  bắt  đầu  bằng  que  thử  thai;  khả  năng  mang  thaicó  kinh  nguyệt  bình  thường  trước  đây.  Nhiều  phụ  nữ  thỉnh  thoảng  bị  mất  kinh  mỗi  tháng.  Vô  kinh  xảy  ra  nếu  người  phụ  nữ  mất  kinh  từ  ba  kỳ  kinh  trở  lên  liên  tiếp.Việc  không  có  kinh  nguyệt  được  gọi  là  amenor-rhea.  Vô  kinh  nguyên  phát  là  tình  trạng  không  bắt  đầu  có  kinhRebecca  J.  FreyVô  kinh  có  thể  có  nhiều  nguyên  nhân.  Vô  kinh  nguyên  phát  có  thể  là  kết  quả  của  sự  mất  cân  bằng  nội  tiết  tố,  rối  loạn  tâm  thần,  rối loạn kinh nguyệt.\\n  Trong  trường hợp  vô kinh nguyên nhân do  buồng trứng  thì  sẽ  xuất hiện  những  triệu chứng sau:  Một số  chị  sử ụng thuốc tránh thai  đều  gặp  ít  n ếu  dùng  trong  ngày  nhất   1 lần  nhưng  vẫn  chưa  hành  hoạt ộng  ảnh ến  trầm ật  gây  khó  chuyển  đổi  sang  khác .  Khi  chu ết  kế  đình  dục  cho  các  anh  em ,  đã  tìm  cách  trị  hiệu  trình  trì  dưỡng  tốt  và  làm  điều ớc  ước  về  vấn  nu ia  sinh sản \\n1. Nguyên nhân của hiện tượng này là do sự')]\n"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     user_input=input(f'Input Prompt:')\n",
    "#     result=get_answer(docsearch, query, model)\n",
    "#     print('Response :', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPTForCausalLM(\n",
       "  (transformer): MPTModel(\n",
       "    (wte): SharedEmbedding(20480, 3072)\n",
       "    (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x MPTBlock(\n",
       "        (norm_1): LPLayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=3072, out_features=9216, bias=True)\n",
       "          (out_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
       "          (down_proj): Linear(in_features=12288, out_features=3072, bias=True)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LPLayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"### Câu hỏi: {instruction}\\n### Trả lời:\"\n",
    "\n",
    "# Some instruction examples\n",
    "# instruction = \"Viết bài văn nghị luận xã hội về {topic}\"\n",
    "# instruction = \"Viết bản mô tả công việc cho vị trí {job_title}\"\n",
    "# instruction = \"Sửa lỗi chính tả:\\n{sentence_or_paragraph}\"\n",
    "# instruction = \"Dựa vào văn bản sau đây:\\n{text}\\nHãy trả lời câu hỏi: {question}\"\n",
    "# instruction = \"Tóm tắt văn bản:\\n{text}\"\n",
    "\n",
    "instruction = \"Sử dụng mẩu thông tin sau và kiến thức của bạn :\\nDị ứng mẩn đỏ và tôi biết nó là ai tôi xin không biết\\n Hãy trả lời câu hỏi: Dị ứng gây ra gì\"\n",
    "# instruction = \"Sửa lỗi chính tả:\\nTriệt phá băng nhóm kướp ô tô, sử dụng \\\"vũ khí nóng\\\"\"\n",
    "\n",
    "input_prompt = PROMPT_TEMPLATE.format_map({\"instruction\": instruction})\n",
    "\n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Move model to CUDA device\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NgLaam\\.cache\\huggingface\\modules\\transformers_modules\\vinai\\PhoGPT-4B-Chat\\116013fa63f8c4025739487e1cbff65b7375bbe2\\attention.py:87: UserWarning: Propagating key_padding_mask to the attention module and applying it within the attention module can cause unnecessary computation/memory usage. Consider integrating into attn_bias once and passing that to each attention module instead.\n",
      "  warnings.warn('Propagating key_padding_mask to the attention module ' + 'and applying it within the attention module can cause ' + 'unnecessary computation/memory usage. Consider integrating ' + 'into attn_bias once and passing that to each attention ' + 'module instead.')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Trả lời:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\transformers\\generation\\utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1589\u001b[0m     )\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1593\u001b[0m         input_ids,\n\u001b[0;32m   1594\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1595\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[0;32m   1596\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1597\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1598\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1599\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1600\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[0;32m   1601\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1602\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1603\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1604\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[0;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1617\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\transformers\\generation\\utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2697\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2698\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2699\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2700\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2701\u001b[0m )\n\u001b[0;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\vinai\\PhoGPT-4B-Chat\\116013fa63f8c4025739487e1cbff65b7375bbe2\\modeling_mpt.py:436\u001b[0m, in \u001b[0;36mMPTForCausalLM.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, prefix_mask, sequence_id, labels, return_dict, output_attentions, output_hidden_states, use_cache, inputs_embeds)\u001b[0m\n\u001b[0;32m    434\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mreturn_dict\n\u001b[0;32m    435\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m use_cache \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m--> 436\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state)\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\vinai\\PhoGPT-4B-Chat\\116013fa63f8c4025739487e1cbff65b7375bbe2\\modeling_mpt.py:357\u001b[0m, in \u001b[0;36mMPTModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, prefix_mask, sequence_id, return_dict, output_attentions, output_hidden_states, use_cache, inputs_embeds)\u001b[0m\n\u001b[0;32m    355\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (x,)\n\u001b[0;32m    356\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[b_idx] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m (x, attn_weights, present) \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb_w_meta_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_emb_w_meta_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflash_attn_padding_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflash_attn_padding_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m presents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     presents \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (present,)\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\vinai\\PhoGPT-4B-Chat\\116013fa63f8c4025739487e1cbff65b7375bbe2\\blocks.py:40\u001b[0m, in \u001b[0;36mMPTBlock.forward\u001b[1;34m(self, x, past_key_value, attn_bias, rotary_emb_w_meta_info, attention_mask, is_causal, output_attentions, alibi_slopes, flash_attn_padding_info)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, past_key_value: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attn_bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rotary_emb_w_meta_info: Optional[Dict]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mByteTensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_causal: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_attentions: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, alibi_slopes: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, flash_attn_padding_info: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[0;32m     39\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_1(x)\n\u001b[1;32m---> 40\u001b[0m     (b, attn_weights, past_key_value) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb_w_meta_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_emb_w_meta_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflash_attn_padding_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflash_attn_padding_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(b)\n\u001b[0;32m     42\u001b[0m     m \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\vinai\\PhoGPT-4B-Chat\\116013fa63f8c4025739487e1cbff65b7375bbe2\\attention.py:278\u001b[0m, in \u001b[0;36mGroupedQueryAttention.forward\u001b[1;34m(self, x, past_key_value, attn_bias, attention_mask, rotary_emb_w_meta_info, is_causal, needs_weights, alibi_slopes, flash_attn_padding_info)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, past_key_value: Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attn_bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rotary_emb_w_meta_info: Optional[\u001b[38;5;28mdict\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_causal: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, needs_weights: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, alibi_slopes: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, flash_attn_padding_info: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[1;32m--> 278\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_qkv:\n\u001b[0;32m    280\u001b[0m         qkv \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_qkv, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_qkv)\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NgLaam\\Desktop\\chatbot\\medical-chatbot\\medical-chatbot\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    inputs=input_ids[\"input_ids\"].to(\"cpu\"),\n",
    "    attention_mask=input_ids[\"attention_mask\"].to(\"cpu\"),\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "response = response.split(\"### Trả lời:\")[1]\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
